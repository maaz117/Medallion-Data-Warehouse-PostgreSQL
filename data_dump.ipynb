{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Using cached psycopg2-2.9.10-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-2.0.38-cp311-cp311-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy)\n",
      "  Using cached greenlet-3.1.1-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\maaza\\onedrive\\desktop\\assignment 6\\.conda\\lib\\site-packages (from sqlalchemy) (4.12.2)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading numpy-2.2.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\maaza\\onedrive\\desktop\\assignment 6\\.conda\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maaza\\onedrive\\desktop\\assignment 6\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached psycopg2-2.9.10-cp311-cp311-win_amd64.whl (1.2 MB)\n",
      "Downloading SQLAlchemy-2.0.38-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.1 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.8/2.1 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.1 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 2.1 MB/s eta 0:00:00\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "Using cached greenlet-3.1.1-cp311-cp311-win_amd64.whl (298 kB)\n",
      "Downloading numpy-2.2.3-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.9 MB 2.8 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.0/12.9 MB 2.6 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.6/12.9 MB 2.8 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.8/12.9 MB 2.7 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 2.1/12.9 MB 2.1 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.9/12.9 MB 2.4 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 3.1/12.9 MB 2.3 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.7/12.9 MB 2.2 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 4.2/12.9 MB 2.2 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 4.5/12.9 MB 2.1 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 4.5/12.9 MB 2.1 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 5.0/12.9 MB 2.1 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 5.2/12.9 MB 1.9 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.5/12.9 MB 1.9 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.5/12.9 MB 1.9 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.8/12.9 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 6.3/12.9 MB 1.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 6.6/12.9 MB 1.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.8/12.9 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 7.3/12.9 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.6/12.9 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 8.1/12.9 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.4/12.9 MB 1.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.9/12.9 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 9.4/12.9 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 10.0/12.9 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 10.2/12.9 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.7/12.9 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.0/12.9 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.3/12.9 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.8/12.9 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.1/12.9 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.6/12.9 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/12.9 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 1.8 MB/s eta 0:00:00\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, python-dotenv, psycopg2, numpy, greenlet, sqlalchemy, pandas\n",
      "Successfully installed greenlet-3.1.1 numpy-2.2.3 pandas-2.2.3 psycopg2-2.9.10 python-dotenv-1.0.1 pytz-2025.1 sqlalchemy-2.0.38 tzdata-2025.1\n"
     ]
    }
   ],
   "source": [
    "! pip install psycopg2 sqlalchemy python-dotenv pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv # for environment variables\n",
    "import psycopg2 # for connecting to postgres\n",
    "from sqlalchemy import create_engine  \n",
    "import pandas as pd\n",
    "\n",
    "# load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# access the environment variables\n",
    "db_user = os.environ.get('POSTGRES_USER')\n",
    "db_password = os.environ.get('POSTGRES_PASSWORD')\n",
    "db_name = os.environ.get('POSTGRES_DB')\n",
    "db_host = 'localhost'\n",
    "db_port = '5432'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres example postgres localhost 5432\n"
     ]
    }
   ],
   "source": [
    "print(db_user, db_password, db_name, db_host, db_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Connect to the database\n",
    "    conn = psycopg2.connect(\n",
    "        host=db_host,\n",
    "        port=db_port,\n",
    "        dbname=db_name,\n",
    "        user=db_user,\n",
    "        password=db_password\n",
    "    )\n",
    "    print(\"Connected to the database successfully!\")\n",
    "except psycopg2.Error as e: \n",
    "    print(f\"Error connecting to the database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f'postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files_directory = './data'\n",
    "if not os.path.exists(csv_files_directory): # check if the directory exists\n",
    "    print(f\"Directory '{csv_files_directory}' does not exist.\")\n",
    "    \n",
    "csv_files = [f for f in os.listdir(csv_files_directory) if f.endswith('.csv')] # get all csv files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CUST_AZ12.csv',\n",
       " 'cust_info.csv',\n",
       " 'LOC_A101.csv',\n",
       " 'prd_info.csv',\n",
       " 'PX_CAT_G1V2.csv',\n",
       " 'sales_details.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CID TEXT,BDATE TEXT,GEN TEXT\n",
      "Creating Table 'CUST_AZ12' with query: \n",
      "            CREATE TABLE IF NOT EXISTS CUST_AZ12 (\n",
      "                CID TEXT,BDATE TEXT,GEN TEXT\n",
      "            )\n",
      "            \n",
      "Data from ./data\\CUST_AZ12.csv has been successfully loaded into CUST_AZ12\n",
      "cst_id TEXT,cst_key TEXT,cst_firstname TEXT,cst_lastname TEXT,cst_marital_status TEXT,cst_gndr TEXT,cst_create_date TEXT\n",
      "Creating Table 'cust_info' with query: \n",
      "            CREATE TABLE IF NOT EXISTS cust_info (\n",
      "                cst_id TEXT,cst_key TEXT,cst_firstname TEXT,cst_lastname TEXT,cst_marital_status TEXT,cst_gndr TEXT,cst_create_date TEXT\n",
      "            )\n",
      "            \n",
      "Data from ./data\\cust_info.csv has been successfully loaded into cust_info\n",
      "CID TEXT,CNTRY TEXT\n",
      "Creating Table 'LOC_A101' with query: \n",
      "            CREATE TABLE IF NOT EXISTS LOC_A101 (\n",
      "                CID TEXT,CNTRY TEXT\n",
      "            )\n",
      "            \n",
      "Data from ./data\\LOC_A101.csv has been successfully loaded into LOC_A101\n",
      "prd_id TEXT,prd_key TEXT,prd_nm TEXT,prd_cost TEXT,prd_line TEXT,prd_start_dt TEXT,prd_end_dt TEXT\n",
      "Creating Table 'prd_info' with query: \n",
      "            CREATE TABLE IF NOT EXISTS prd_info (\n",
      "                prd_id TEXT,prd_key TEXT,prd_nm TEXT,prd_cost TEXT,prd_line TEXT,prd_start_dt TEXT,prd_end_dt TEXT\n",
      "            )\n",
      "            \n",
      "Data from ./data\\prd_info.csv has been successfully loaded into prd_info\n",
      "ID TEXT,CAT TEXT,SUBCAT TEXT,MAINTENANCE TEXT\n",
      "Creating Table 'PX_CAT_G1V2' with query: \n",
      "            CREATE TABLE IF NOT EXISTS PX_CAT_G1V2 (\n",
      "                ID TEXT,CAT TEXT,SUBCAT TEXT,MAINTENANCE TEXT\n",
      "            )\n",
      "            \n",
      "Data from ./data\\PX_CAT_G1V2.csv has been successfully loaded into PX_CAT_G1V2\n",
      "sls_ord_num TEXT,sls_prd_key TEXT,sls_cust_id TEXT,sls_order_dt TEXT,sls_ship_dt TEXT,sls_due_dt TEXT,sls_sales TEXT,sls_quantity TEXT,sls_price TEXT\n",
      "Creating Table 'sales_details' with query: \n",
      "            CREATE TABLE IF NOT EXISTS sales_details (\n",
      "                sls_ord_num TEXT,sls_prd_key TEXT,sls_cust_id TEXT,sls_order_dt TEXT,sls_ship_dt TEXT,sls_due_dt TEXT,sls_sales TEXT,sls_quantity TEXT,sls_price TEXT\n",
      "            )\n",
      "            \n",
      "Data from ./data\\sales_details.csv has been successfully loaded into sales_details\n",
      "PostgreSQL connection closed\n"
     ]
    }
   ],
   "source": [
    "def load_csv_to_postgresql(csv_files, table_name):\n",
    "   try: \n",
    "        df = pd.read_csv(csv_files) # read csv file\n",
    "        columns = \",\".join([f\"{cols} TEXT\" for cols in df.columns]) # Read coulumns give text data type and join # to string\n",
    "        print(columns)\n",
    "        create_table_query = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                {columns}\n",
    "            )\n",
    "            \"\"\" # create table\n",
    "            \n",
    "        print(f\"Creating Table '{table_name}' with query: {create_table_query}\")  # print create table query\n",
    "        \n",
    "        with conn.cursor() as cursor: # we don't have to write exit using ' with '\n",
    "            cursor.execute(create_table_query) # execute create table query\n",
    "            conn.commit() # commit changes\n",
    "            \n",
    "        df.to_sql(table_name, engine, if_exists='replace', index=False, method='multi', chunksize=1000) # insert data into table\n",
    "        print(f\"Data from {csv_files} has been successfully loaded into {table_name}\")\n",
    "        \n",
    "   except Exception as e:\n",
    "       print(f\"Error loading data from {csv_files} into {table_name}: {e}\")    \n",
    "       \n",
    "for csv_file in csv_files: # loop through csv files\n",
    "    csv_file_path = os.path.join(csv_files_directory, csv_file) # get csv file path\n",
    "    table_name = os.path.splitext(csv_file)[0] # get table name\n",
    "    \n",
    "    load_csv_to_postgresql(csv_file_path, table_name) # call function\n",
    "    \n",
    "conn.close()\n",
    "print(\"PostgreSQL connection closed\")        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
